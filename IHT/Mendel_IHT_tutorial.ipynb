{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenMendel Tutorial on Iterative Hard Thresholding\n",
    "\n",
    "### Last update: 4/24/2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Julia version\n",
    "\n",
    "`MendelIHT.jl` currently supports Julia 1.0 and 1.1, but it currently an unregistered package. To install, press `]` to invoke the package manager mode and install these packages by typing:\n",
    "\n",
    "```\n",
    "add https://github.com/OpenMendel/SnpArrays.jl\n",
    "add https://github.com/OpenMendel/MendelSearch.jl\n",
    "add https://github.com/OpenMendel/MendelBase.jl\n",
    "add https://github.com/biona001/MendelIHT.jl\n",
    "```\n",
    "\n",
    "For this tutorial you will also need a few registered packages. Add them by typing:\n",
    "\n",
    "```\n",
    "add DataFrames, Distributions, BenchmarkTools, Random, LinearAlgebra, GLM\n",
    "```\n",
    "\n",
    "For reproducibility, the computer spec and Julia version is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.0.3\n",
      "Commit 099e826241 (2018-12-18 01:34 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin14.5.0)\n",
      "  CPU: Intel(R) Core(TM) i7-3740QM CPU @ 2.70GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-6.0.0 (ORCJIT, ivybridge)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Iterative Hard Thresholding\n",
    "\n",
    "Continuous model selection is advantageous in situations where the multivariate nature of the regressors plays a significant role together. As an alternative to traditional SNP-by-SNP association testing, iterative hard-thresholing (IHT) performs continuous model selection on a GWAS dataset $\\mathbf{X} \\in \\{0, 1, 2\\}^{n \\times p}$ and continuous phenotype vector $\\mathbf{y}$ by maximizing the loglikelihood $L(\\beta)$ subject to the constraint that $\\beta$ is $k-$sparse. This method has the edge over LASSO because IHT does not shrink estimated effect sizes. Parallel computing is offered through `q-`fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appropriate Datasets and Example Inputs \n",
    "\n",
    "All genotype data **must** be stored in the [PLINK binary genotype format](https://www.cog-genomics.org/plink2/formats#bed) where at least the triplets `.bim`, `.bed` and `.fam` must all be present. Additional non-genetic covariates should be imported separately by the user. In the examples below, we first simulate phenotypes from the Normal, Bernoulli, Poisson, and Negative Binomial family, and then attempt to fit the corresponding model using our IHT implementation. We can examine reconstruction behavior as well as the ability for cross validation to find the true sparsity parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "`MendelIHT` assumes there are no missing genotypes, since it uses linear algebra functions defined in [SnpArrays.jl](https://openmendel.github.io/SnpArrays.jl/latest/man/snparray/#linear-algebra-with-snparray). Therefore, you must first impute missing genotypes before you use MendelIHT. SnpArrays.jl offer basic quality control routines such as filtering, but otherwise, our own software [option 23 of Mendel](http://software.genetics.ucla.edu/download?package=1) is a reasonable choice. Open Mendel will soon provide a separate package `MendelImpute.jl` containing new imputation strategies such as alternating least squares.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and Regularization paths\n",
    "\n",
    "We usually have very little information on how many SNPs are affecting the phenotype. In a typical GWAS study, anywhere between 1 to thousands of SNPs could play a role. Thus ideally, we can test many different models to find the best one. MendelIHT provides 2 ways for one to perform this automatically: user specified regulartization paths, and $q-$fold [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics). Users should know that, in the first method, increasing the number of predictors will almost always decrease the error, but as a result introduce overfitting. Therefore, in most practical situations, it is highly recommended to combine this method with cross validation. In $q-$fold cross validation, samples are divided into $q$ disjoint subsets, and IHT fits a model on $q-1$ of those sets data, then computes the [MSE](https://en.wikipedia.org/wiki/Mean_squared_error) tested on the $qth$ samples. Each $q$ subsets are served as the test set exactly once. This functionality of `MendelIHT.jl` natively supports parallel computing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Quantitative Traits\n",
    "\n",
    "Quantitative traits are continuous phenotypes that can essentially take on any real number. In this example, we first simulate $y_i \\sim x_i^T\\beta + \\epsilon_i$ where $\\epsilon_i \\sim N(0, 1)$ and $\\beta_i \\sim N(0, 1)$. Then using just the genotype matrix $X$ and phenotype vector $y$, we use IHT to recover the simulated $\\beta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first add workers needed for parallel computing. Add only as many CPU cores you have \n",
    "using Distributed\n",
    "addprocs(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load necessary packages\n",
    "using MendelIHT\n",
    "using SnpArrays\n",
    "using DataFrames\n",
    "using Distributions\n",
    "using BenchmarkTools\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Simulat data with k true predictors, from distribution d and with link l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IdentityLink()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1000\n",
    "p = 10000\n",
    "k = 10\n",
    "d = Normal\n",
    "l = canonicallink(d())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Construct snpmatrix, covariate files, and true model b\n",
    "\n",
    "The SnpBitMatrix type (`xbm` below) is necessary for performing linear algebra directly on raw genotype files without expanding the matrix to numeric floating points. Here the SnpArray (`x` below) is memory-mapped to a file called `tmp.bed` stored on your disk, and hence, does not require RAM to store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1 Array{Float64,2}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " ⋮  \n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(1111) #set random seed\n",
    "x, = simulate_random_snparray(n, p, \"tmp.bed\")\n",
    "xbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true); \n",
    "z = ones(n, 1) # only nongenetic covariate is the intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Simulate response y, true model b, and the correct non-0 positions of b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-2.02168, -2.56256, 1.2439, 0.304348, 1.70433, -2.7755, -0.948664, 0.166054, 1.58801, 1.03323  …  -2.1665, 7.97552, 0.324306, 1.60573, 1.59093, -2.50396, -3.46523, -0.346403, 1.07067, 0.292686], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [2384, 3352, 4093, 5413, 5455, 6729, 7403, 8753, 9089, 9132])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, true_b, correct_position = simulate_random_response(x, xbm, k, d, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run IHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IHT results:\n",
       "\n",
       "Compute time (sec):     0.8107540607452393\n",
       "Final loglikelihood:    -1407.2533232402275\n",
       "Iterations:             12\n",
       "Max number of groups:   1\n",
       "Max predictors/group:   10\n",
       "IHT estimated 10 nonzero SNP predictors and 0 non-genetic predictors.\n",
       "\n",
       "Selected genetic predictors:\n",
       "10×2 DataFrame\n",
       "│ Row │ Position │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼──────────┼─────────────┤\n",
       "│ 1   │ 2384     │ -1.26014    │\n",
       "│ 2   │ 3352     │ -0.26742    │\n",
       "│ 3   │ 3353     │ 0.141208    │\n",
       "│ 4   │ 4093     │ 0.289956    │\n",
       "│ 5   │ 5413     │ 0.366689    │\n",
       "│ 6   │ 5609     │ -0.137181   │\n",
       "│ 7   │ 7403     │ -0.308255   │\n",
       "│ 8   │ 8753     │ 0.332881    │\n",
       "│ 9   │ 9089     │ 0.964598    │\n",
       "│ 10  │ 9132     │ -0.509461   │\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "0×2 DataFrame\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = L0_reg(x, xbm, z, y, 1, k, d(), l, debias=false, init=false, use_maf=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Check results\n",
    "\n",
    "IHT found 8/10 predictors in this example. The 2 that was not found had a relatively small effect size, and as far as IHT can tell, they are indistinguishable from noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model = 10×2 DataFrame\n",
      "│ Row │ true_β    │ estimated_β │\n",
      "│     │ Float64   │ Float64     │\n",
      "├─────┼───────────┼─────────────┤\n",
      "│ 1   │ -1.19376  │ -1.26014    │\n",
      "│ 2   │ -0.230351 │ -0.26742    │\n",
      "│ 3   │ 0.257181  │ 0.289956    │\n",
      "│ 4   │ 0.344827  │ 0.366689    │\n",
      "│ 5   │ 0.155484  │ 0.0         │\n",
      "│ 6   │ -0.126114 │ 0.0         │\n",
      "│ 7   │ -0.286079 │ -0.308255   │\n",
      "│ 8   │ 0.327039  │ 0.332881    │\n",
      "│ 9   │ 0.931375  │ 0.964598    │\n",
      "│ 10  │ -0.496683 │ -0.509461   │\n",
      "Total iteration number was 12\n",
      "Total time was 0.8107540607452393\n",
      "Total found predictors = 8\n"
     ]
    }
   ],
   "source": [
    "compare_model = DataFrame(\n",
    "    true_β      = true_b[correct_position], \n",
    "    estimated_β = result.beta[correct_position])\n",
    "@show compare_model\n",
    "println(\"Total iteration number was \" * string(result.iter))\n",
    "println(\"Total time was \" * string(result.time))\n",
    "println(\"Total found predictors = \" * string(length(findall(!iszero, result.beta[correct_position]))))\n",
    "\n",
    "#clean up\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Case-control study controlling for sex\n",
    "\n",
    "Case control studies are used when the phenotype in a binary count data. In this example, we simulate a case-control study, while controling for sex as a non-genetic covariate. \n",
    "\n",
    "The exact simulation code to generate the phenotype $y$ can be found at: https://github.com/biona001/MendelIHT.jl/blob/master/src/simulate_utilities.jl#L107"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Simulat data with k true predictors, from distribution d and with link l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogitLink()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 1000\n",
    "p = 10000\n",
    "k = 10\n",
    "d = Bernoulli\n",
    "l = canonicallink(d())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: construct snpmatrix, covariate files, and true model b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000-element view(::Array{Float64,2}, :, 2) with eltype Float64:\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0\n",
       " 1.0\n",
       " 0.0\n",
       " 1.0\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0\n",
       " 1.0\n",
       " ⋮  \n",
       " 0.0\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 1.0\n",
       " 0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(1111) #set random seed \n",
    "x, = simulate_random_snparray(n, p, \"tmp.bed\")\n",
    "xbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true); \n",
    "z = ones(n, 2) # first column is the intercept, second column the sex. 0 = male 1 = female\n",
    "z[:, 2] .= rand(0:1, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: simulate true models \n",
    "\n",
    "Here we used $k=8$ genetic predictors and 2 non-genetic predictors (intercept and sex). The simulation code in our package does not yet handle simulations with non-genetic predictors, so we must simulate these phenotypes manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 1.0\n",
       " 1.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_b = zeros(p) #genetic predictors\n",
    "true_b[1:k-2] = randn(k-2)\n",
    "shuffle!(true_b)\n",
    "correct_position = findall(!iszero, true_b)\n",
    "true_c = [1.0; 1.5] #non-genetic predictors: intercept & sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: simulate phenotype using genetic and nongenetic predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000-element Array{Float64,1}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 0.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0\n",
       " 1.0\n",
       " 0.0\n",
       " ⋮  \n",
       " 0.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 0.0\n",
       " 1.0\n",
       " 0.0\n",
       " 0.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = linkinv.(l, xbm * true_b .+ z * true_c)\n",
    "y = [rand(d(i)) for i in prob]\n",
    "y = Float64.(y) #convert y to floating point numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: run IHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IHT results:\n",
       "\n",
       "Compute time (sec):     2.4853720664978027\n",
       "Final loglikelihood:    -286.3534665608417\n",
       "Iterations:             48\n",
       "Max number of groups:   1\n",
       "Max predictors/group:   10\n",
       "IHT estimated 8 nonzero SNP predictors and 2 non-genetic predictors.\n",
       "\n",
       "Selected genetic predictors:\n",
       "8×2 DataFrame\n",
       "│ Row │ Position │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼──────────┼─────────────┤\n",
       "│ 1   │ 98       │ 0.36567     │\n",
       "│ 2   │ 983      │ 0.424646    │\n",
       "│ 3   │ 2960     │ -2.26025    │\n",
       "│ 4   │ 4461     │ 0.427904    │\n",
       "│ 5   │ 4588     │ -0.67782    │\n",
       "│ 6   │ 6086     │ 0.777424    │\n",
       "│ 7   │ 6130     │ -0.948359   │\n",
       "│ 8   │ 9283     │ -0.753285   │\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "2×2 DataFrame\n",
       "│ Row │ Position │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼──────────┼─────────────┤\n",
       "│ 1   │ 1        │ 0.980127    │\n",
       "│ 2   │ 2        │ 1.71288     │"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = L0_reg(x, xbm, z, y, 1, k, d(), l, debias=false, init=false, use_maf=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: check result\n",
    "\n",
    "As we can see below, IHT finds 5/8 true genetic predictors and 2/2 true non-genetic predictors. Note that:\n",
    "\n",
    "+ The coefficient estimates for found predictors are unbiased.\n",
    "+ Larger effect sizes are easier to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model_genetic = 8×2 DataFrame\n",
      "│ Row │ true_β    │ estimated_β │\n",
      "│     │ Float64   │ Float64     │\n",
      "├─────┼───────────┼─────────────┤\n",
      "│ 1   │ -2.22637  │ -2.26025    │\n",
      "│ 2   │ 0.0646127 │ 0.0         │\n",
      "│ 3   │ -0.63696  │ -0.67782    │\n",
      "│ 4   │ 1.08631   │ 0.777424    │\n",
      "│ 5   │ -0.930103 │ -0.948359   │\n",
      "│ 6   │ -0.283783 │ 0.0         │\n",
      "│ 7   │ -0.206074 │ 0.0         │\n",
      "│ 8   │ -0.553461 │ -0.753285   │\n",
      "\n",
      "\n",
      "compare_model_nongenetic = 2×2 DataFrame\n",
      "│ Row │ true_c  │ estimated_c │\n",
      "│     │ Float64 │ Float64     │\n",
      "├─────┼─────────┼─────────────┤\n",
      "│ 1   │ 1.0     │ 0.980127    │\n",
      "│ 2   │ 1.5     │ 1.71288     │\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>true_c</th><th>estimated_c</th></tr><tr><th></th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>2 rows × 2 columns</p><tr><th>1</th><td>1.0</td><td>0.980127</td></tr><tr><th>2</th><td>1.5</td><td>1.71288</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& true\\_c & estimated\\_c\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1.0 & 0.980127 \\\\\n",
       "\t2 & 1.5 & 1.71288 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "2×2 DataFrame\n",
       "│ Row │ true_c  │ estimated_c │\n",
       "│     │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼─────────┼─────────────┤\n",
       "│ 1   │ 1.0     │ 0.980127    │\n",
       "│ 2   │ 1.5     │ 1.71288     │"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_model_genetic = DataFrame(\n",
    "    true_β      = true_b[correct_position], \n",
    "    estimated_β = result.beta[correct_position])\n",
    "\n",
    "compare_model_nongenetic = DataFrame(\n",
    "    true_c      = true_c[1:2], \n",
    "    estimated_c = result.c[1:2])\n",
    "\n",
    "@show compare_model_genetic\n",
    "println(\"\\n\")\n",
    "@show compare_model_nongenetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Cross Validation with Poisson using debiasing\n",
    "\n",
    "In this example, we investiate IHT's cross validation routines using as many CPU cores as possible. We use Poisson regression as an example. The current machine (4 cores avaialble) info is listed in the beginning of this tutorial. We also turned on debiasing just to show that this functionality work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Verify we can multiple workers involved. \n",
    "\n",
    "Workers were added in the first example with the Distributed.jl package. If `nprocs()` return 1, restart the notebook and add workers before loading packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nprocs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: simulat data with k true predictors, from distribution d and with link l.\n",
    "\n",
    "Here we chose a larger sample size to have better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogLink()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 5000\n",
    "p = 30000\n",
    "k = 10\n",
    "d = Poisson\n",
    "l = canonicallink(d())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: construct snpmatrix, covariate files, and true model b\n",
    "\n",
    "Note using `undef` as the third argument will instead create non-memory mapped SnpArrays, which must be stored in the RAM. While this has extra memory overhead, it also facilitates quicker data access. Therefore it is on the user to decide when it is appropriate to create memory mapped files and when it is not. If one is not very computer savvy, we recommend always doing memory mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000×1 Array{Float64,2}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " ⋮  \n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Random.seed!(1111) #set random seed\n",
    "x, = simulate_random_snparray(n, p, undef)\n",
    "xbm = SnpBitMatrix{Float64}(x, model=ADDITIVE_MODEL, center=true, scale=true); \n",
    "z = ones(n, 1) # the intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: simulate response, true model b, and the correct non-0 positions of b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0, 6.0, 0.0, 0.0, 6.0, 1.0, 1.0, 0.0, 0.0, 7.0  …  1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 12.0, 1.0, 2.0, 9.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1744, 6495, 10765, 12333, 16133, 17026, 17885, 21068, 22330, 29911])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, true_b, correct_position = simulate_random_response(x, xbm, k, d, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: specify path and folds\n",
    "\n",
    "Here `path` are all the model sizes you wish to test and `folds` indicates how to partition the samples into disjoint groups. It is important we partition the training/testing data randomly as opposed to chunck by chunck to avoid nasty things like sampling biases. Below we tested $k = 1, 2, ..., 20$ across 3 fold. This is equivalent to running IHT across 60 different models, and hence, is ideal for parallel computing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 2\n",
       " 3\n",
       " 2\n",
       " 3\n",
       " 1\n",
       " 2\n",
       " 2\n",
       " 3\n",
       " 2\n",
       " 1\n",
       " 1\n",
       " ⋮\n",
       " 2\n",
       " 3\n",
       " 3\n",
       " 2\n",
       " 3\n",
       " 3\n",
       " 1\n",
       " 3\n",
       " 2\n",
       " 2\n",
       " 3\n",
       " 2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = collect(1:20)\n",
    "num_folds = 3\n",
    "folds = rand(1:num_folds, size(x, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Run IHT's cross validation routine\n",
    "\n",
    "This returns a vector of deviance residuals, which is a generalization of the mean squared error. \n",
    "\n",
    "**Warning:** This step will generate intermediate files with similar titles as `train.tmp` and `test.tmp`. These are necessary auxiliary files that will be automatically removed when cross validation completes. **Removing these files before the algorithm terminate will lead of bad errors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "\tk\tMSE\n",
      "\t1\t5581.143071419869\n",
      "\t2\t4223.403648644891\n",
      "\t3\t3239.104084235584\n",
      "\t4\t2879.8855975535353\n",
      "\t5\t2575.1195191721963\n",
      "\t6\t2315.6174299543\n",
      "\t7\t2230.341919415393\n",
      "\t8\t1948.4954290886512\n",
      "\t9\t1830.8021581831804\n",
      "\t10\t1678.88047182858\n",
      "\t11\t1695.310144524478\n",
      "\t12\t1706.3305709652873\n",
      "\t13\t1710.8323819723923\n",
      "\t14\t1719.5785652367854\n",
      "\t15\t1722.8525298462273\n",
      "\t16\t1730.1535563414525\n",
      "\t17\t1728.6745696861462\n",
      "\t18\t1734.1884767964175\n",
      "\t19\t1738.055904325496\n",
      "\t20\t1741.9157245269835\n",
      "\n",
      "The lowest MSE is achieved at k = 10 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20-element Array{Float64,1}:\n",
       " 5581.143071419869 \n",
       " 4223.403648644891 \n",
       " 3239.104084235584 \n",
       " 2879.8855975535353\n",
       " 2575.1195191721963\n",
       " 2315.6174299543   \n",
       " 2230.341919415393 \n",
       " 1948.4954290886512\n",
       " 1830.8021581831804\n",
       " 1678.88047182858  \n",
       " 1695.310144524478 \n",
       " 1706.3305709652873\n",
       " 1710.8323819723923\n",
       " 1719.5785652367854\n",
       " 1722.8525298462273\n",
       " 1730.1535563414525\n",
       " 1728.6745696861462\n",
       " 1734.1884767964175\n",
       " 1738.055904325496 \n",
       " 1741.9157245269835"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drs = cv_iht(d(), l, x, z, y, 1, path, folds, num_folds, debias=true, parallel=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Run full model on the best estimated model size \n",
    "\n",
    "According to our cross validation result, the best model size that minimizes deviance residuals (i.e. MSE on the q-th subset of samples) is attained at $k = 10$. That is, cross validation detected that we need 10 SNPs to achieve the best model size. Using this information, one can re-run the IHT code to obtain the estimated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IHT results:\n",
       "\n",
       "Compute time (sec):     24.928004026412964\n",
       "Final loglikelihood:    -6720.822219489936\n",
       "Iterations:             30\n",
       "Max number of groups:   1\n",
       "Max predictors/group:   10\n",
       "IHT estimated 10 nonzero SNP predictors and 0 non-genetic predictors.\n",
       "\n",
       "Selected genetic predictors:\n",
       "10×2 DataFrame\n",
       "│ Row │ Position │ Estimated_β │\n",
       "│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │\n",
       "├─────┼──────────┼─────────────┤\n",
       "│ 1   │ 1744     │ 0.842532    │\n",
       "│ 2   │ 6495     │ 0.179672    │\n",
       "│ 3   │ 10765    │ 0.183316    │\n",
       "│ 4   │ 12333    │ -0.236543   │\n",
       "│ 5   │ 16133    │ -0.254481   │\n",
       "│ 6   │ 17026    │ 0.462852    │\n",
       "│ 7   │ 17885    │ -0.299888   │\n",
       "│ 8   │ 21068    │ -0.222807   │\n",
       "│ 9   │ 22330    │ 0.202813    │\n",
       "│ 10  │ 29911    │ -0.561777   │\n",
       "\n",
       "Selected nongenetic predictors:\n",
       "0×2 DataFrame\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_est = argmin(drs)\n",
    "result = L0_reg(x, xbm, z, y, 1, k_est, d(), l, debias=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Check final model against simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare_model = 10×2 DataFrame\n",
      "│ Row │ true_β    │ estimated_β │\n",
      "│     │ Float64   │ Float64     │\n",
      "├─────┼───────────┼─────────────┤\n",
      "│ 1   │ 0.842304  │ 0.842532    │\n",
      "│ 2   │ 0.162081  │ 0.179672    │\n",
      "│ 3   │ 0.165261  │ 0.183316    │\n",
      "│ 4   │ -0.23034  │ -0.236543   │\n",
      "│ 5   │ -0.253494 │ -0.254481   │\n",
      "│ 6   │ 0.472102  │ 0.462852    │\n",
      "│ 7   │ -0.315671 │ -0.299888   │\n",
      "│ 8   │ -0.216082 │ -0.222807   │\n",
      "│ 9   │ 0.199723  │ 0.202813    │\n",
      "│ 10  │ -0.547869 │ -0.561777   │\n",
      "Total iteration number was 30\n",
      "Total time was 24.928004026412964\n",
      "Total found predictors = 10\n"
     ]
    }
   ],
   "source": [
    "compare_model = DataFrame(\n",
    "    true_β      = true_b[correct_position], \n",
    "    estimated_β = result.beta[correct_position])\n",
    "@show compare_model\n",
    "println(\"Total iteration number was \" * string(result.iter))\n",
    "println(\"Total time was \" * string(result.time))\n",
    "println(\"Total found predictors = \" * string(length(findall(!iszero, result.beta[correct_position]))))\n",
    "\n",
    "#clean up\n",
    "rm(\"tmp.bed\", force=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook demonstrated some of the basic features of IHT. It is important to note that in the real world, the effect sizes of genetic predictors are expected to be small. Thus to detecting them would require a reasonably large sample size (say $n$ in the thousands). Fortunately, this is common place nowadays. \n",
    "\n",
    "\n",
    "# Extra features \n",
    "\n",
    "Due to limited space, we obmited illustrating some functionalities that have already been implemented, listed below:\n",
    "\n",
    "+ Negative binomial, gamma, inverse gaussian, and binomial regressions\n",
    "+ Use of non-canonical link functions \n",
    "+ Initializing IHT at a good starting point (setting init=true)\n",
    "+ Doubly sparse projection (requires group information)\n",
    "+ Weighted projections (requires weight information)\n",
    "\n",
    "Interested users can visit [our code to reproduce certain figures of our paper](https://github.com/biona001/MendelIHT.jl/tree/master/figures) on our github. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
