{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenMendel: Iterative Hard Thresholding Tutorial\n",
    "\n",
    "### Last update: 11/3/2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Julia version\n",
    "\n",
    "For reproducibility, the computer spec and Julia version is listed below. Current code supports Julia version 0.6.4. It will not work in v0.7 or v1.0, but upgrade to v1.0 is very high on our TODO list. \n",
    "\n",
    "The IHT.jl module must be installed on your computer before running this tutorial.  Instructions on cloning and the latest IHT code can be found here: https://github.com/biona001/IHT.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 0.6.4\n",
      "Commit 9d11f62bcb (2018-07-09 19:09 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin14.5.0)\n",
      "  CPU: Intel(R) Core(TM) i7-3740QM CPU @ 2.70GHz\n",
      "  WORD_SIZE: 64\n",
      "  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Sandybridge MAX_THREADS=16)\n",
      "  LAPACK: libopenblas64_\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-3.9.1 (ORCJIT, ivybridge)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use Iterative Hard Thresholding\n",
    "\n",
    "*Continuous* model selection is advantageous in situations where the multivariate nature of the regressors plays a significant role *together*. As an alternative to traditional SNP-by-SNP association testing, iterative hard-thresholing (IHT) performs continuous model selection on a GWAS dataset $\\mathbf{X} \\in \\{0, 1, 2\\}^{n \\times p}$ and continuous phenotype vector $\\mathbf{y}$ by minimizing the residual sum of squares $f(\\beta) = \\frac{1}{2}||\\mathbf{y} - \\mathbf{X}\\beta||^2$ subject to the constraint that $\\beta$ is $k-$sparse. This method has the edge over LASSO (which also provides continuous model selection) because IHT does not shrink estimated effect sizes. Parallel computing is offered through `q-`fold cross validation, and in the near future, dense (genotype matrix)-(dense vector) multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appropriate Datasets and Example Inputs \n",
    "\n",
    "All genotype data **must** be stored in the [PLINK binary genotype format](https://www.cog-genomics.org/plink2/formats#bed), where the triplets `.bim`, `.bed` and `.fam` must all be present. Additional non-genetic covariates should be stored in a separate file (e.g. comma separated file). In the first 3 examples of this tutorial, we use \"gwas 1 data\" (github repo: [here](https://github.com/OpenMendel/MendelGWAS.jl/tree/master/docs)) to illustrate the basic usage and functionalities of MendelIHT. This dataset has 2200 people and a modest 10000 simulated SNPs, with 2 SNPs (`rs1935681` and `rs2256412`) contributing to the response. One can obtain this dataset from the first example input of [MendelGWAS.jl](https://openmendel.github.io/MendelGWAS.jl/), or via option 24a of the free application [Mendel version 16](http://www.genetics.ucla.edu/software/mendel). To examine the robustness and accuracy of IHT, examples 4 and 5 simulates data on the spot, and immediately calls IHT on the simulated data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "\n",
    "`MendelIHT` assumes there are no missing genotypes, since it uses linear algebra functions defined in [SnpArrays.jl](https://openmendel.github.io/SnpArrays.jl/latest/man/snparray/#linear-algebra-with-snparray). Therefore, you must first impute missing genotypes before you use MendelIHT. SnpArrays.jl offer a naive imputation strategy, but otherwise, our own software [option 23 of Mendel](http://www.genetics.ucla.edu/software/mendel) is a reasonable choice. Open Mendel will soon provide a separate package `MendelImpute` containing new imputation strategies such as alternating least squares.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and Regularization paths\n",
    "\n",
    "We usually have very little information on how many SNPs are affecting the phenotype. In a typical GWAS study, anywhere between 1 to thousands of SNPs could play a role. Thus ideally, we can test many different models to find the best one. MendelIHT provides 2 ways for one to perform this automatically: user specified regulartization paths, and $q-$fold [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics). Users should know that, in the first method, increasing the number of predictors will almost always decrease the error, but as a result introduce overfitting. Therefore, in most practical situations, it is highly recommended to combine this method with cross validation. In $q-$fold cross validation, samples are divided into $q$ disjoint subsets, and IHT fits a model on $q-1$ of those sets data, then computes the [MSE](https://en.wikipedia.org/wiki/Mean_squared_error) tested on the $qth$ samples. Each $q$ subsets are served as the test set exactly once. With noisy responses, IHT cross validation typically underestimates the true model size, as shown in the examples below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis keywords available to users \n",
    "\n",
    "| Keyword | Default Value | Allowed value | Description |\n",
    "| --- | --- | --- | --- |\n",
    "|`predictors` | 0 | Positive integer | Max number of non-zero entries of $\\beta$ |\n",
    "|`non_genetic_covariates` | \"\" | File name on disk | Delimited file containing the non-genetic covariates for each sample |\n",
    "|`run_cross_validation` | false | boolean | Whether the user wants to run cross-validation |\n",
    "|`model_sizes` | \"\" | Integers stored in string separated by ',' | Different model sizes users wish to run IHT |\n",
    "|`cv_fold` | 0 | Positive integer | Number of disjoint subsets the samples should be divided into |\n",
    "|`max_groups` (\\*) | 1 | Integer | Total number of groups |\n",
    "|`group_membership` (\\*) | \"\" | File name on disk | File indicating group membership |\n",
    "|`prior_weights` (\\*) | \"\" | maf | How to scale predictors based on different weights |\n",
    "\n",
    "+ (\\*) Indicates experimental features. We currently have no theoretical guarantees on their performance, therefore illustrations of these functionalities are omitted from this tutorial. Users should tread carefully with these features. \n",
    "+ A list of OpenMendel keywords common to most analysis package can be found [here](https://openmendel.github.io/MendelBase.jl/#keywords-table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Run IHT with Only Genotype Data\n",
    "\n",
    "### Step 1: Preparing Input files\n",
    "\n",
    "In Open Mendel, all analysis parameters are specified via the [Control file](https://openmendel.github.io/MendelBase.jl/#control-file). Genotype data must be inputted via the PLINK binary format. The most basic control file to run IHT looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# Input and Output files.\n",
      "#\n",
      "plink_input_basename = gwas 1 data\n",
      "\n",
      "#\n",
      "# Analysis parameters for IHT option.\n",
      "#\n",
      "predictors = 2"
     ]
    }
   ],
   "source": [
    ";cat \"gwas 1 Control basic.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run MendelIHT\n",
    "\n",
    "To run `MendelIHT`, execute the following in the Julia REPL or in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "     Welcome to OpenMendel's\n",
      "      IHT analysis option\n",
      "        version 0.4.0\n",
      " \n",
      " \n",
      "Reading the data.\n",
      "\n",
      "The current working directory is \"/Users/biona001/.julia/v0.6/IHT/docs/MendelIHT_tutorial\".\n",
      "\n",
      "Keywords modified by the user:\n",
      "\n",
      "  affected_designator = 2\n",
      "  control_file = gwas 1 Control basic.txt\n",
      "  pedigree_file = gwas 1 data.fam\n",
      "  plink_input_basename = gwas 1 data\n",
      "  predictors = 2\n",
      "  snpdata_file = gwas 1 data.bed\n",
      "  snpdefinition_file = gwas 1 data.bim\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mReading in data\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mv1.0 BED file detected\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mAnalyzing the data for model size k = 2\n",
      "\u001b[39m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IHT results:\n",
       "\n",
       "Compute time (sec):     1.477848303\n",
       "Final loss:             1183.6890803583537\n",
       "Iterations:             4\n",
       "Max number of groups:   1\n",
       "Max predictors/group:   2\n",
       "IHT estimated 2 nonzero coefficients.\n",
       "2×3 DataFrames.DataFrame\n",
       "│ Row │ Group │ Predictor │ Estimated_β │\n",
       "├─────┼───────┼───────────┼─────────────┤\n",
       "│ 1   │ 1     │ 3981      │ 0.149073    │\n",
       "│ 2   │ 1     │ 7023      │ 0.272991    │\n",
       "\n",
       "Intercept of model = 0.0\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using IHT\n",
    "MendelIHT(\"gwas 1 Control basic.txt\") # change directory as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 3: Interpreting the results\n",
    "\n",
    "Here the estimated model is the 3981th and 7023th predictor, corresponding to rs1935681 and rs2256412 in the `gwas 1 data.bim` file, which are the correct SNPs. The intercept of the model is given at the bottom of the table. Here the compute time is the time associated with computing the optimal $\\beta$ only, so it does not include other necessary processes, such as importing the data. \n",
    "\n",
    "**Note:** the `affected_designator = 2` simply indicates that the pedigree is a Plink .fam file, which must always be the case for `MendelIHT` because this analysis option only accepts PLINK binary format as inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Including Non-Genetic Covariates\n",
    "\n",
    "Non-genetic covariates must be stored in a comma demited file, with the same number of rows as the number of samples. The intercept term (i.e. grand mean) must also be included in the file. If the user does not specify a non-genetic covariate file, `MendelIHT` will by default include an intercept in the estimated model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Non-Genetic Covariate File\n",
    "\n",
    "In this example, we generated one non-genetic covariate from a $N(0, 1)$ distribution. After including the grand mean, we saved the file in `gwas 1 noncov.txt` where the entries are separated by a tab. The first few lines of this file looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t-0.088704513339476\n",
      "1\t-0.9575873240069772\n",
      "1\t-0.9713258274139007\n",
      "1\t-0.9847900613424241\n",
      "1\t-0.5954781589540936\n",
      "1\t0.2124813875751884\n",
      "1\t2.28150775802523\n",
      "1\t1.7643235366779797\n",
      "1\t-0.3933262467789896\n",
      "1\t-0.1348394065324508\n"
     ]
    }
   ],
   "source": [
    ";head -10 \"gwas 1 noncov.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Corresponding Control File \n",
    "\n",
    "We need to tell MendelIHT that the covariates are separated by tabs. This can be specified via the [MendelBase](https://openmendel.github.io/MendelBase.jl/) keyword `field_separator` in the control file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# Input and Output files.\n",
      "#\n",
      "plink_input_basename = gwas 1 data\n",
      "non_genetic_covariates = gwas 1 noncov.txt\n",
      "field_separator = '\t'\n",
      "#\n",
      "# Analysis parameters for IHT option.\n",
      "#\n",
      "predictors = 2"
     ]
    }
   ],
   "source": [
    ";cat \"gwas 1 Control nongen.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run IHT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "     Welcome to OpenMendel's\n",
      "      IHT analysis option\n",
      "        version 0.4.0\n",
      " \n",
      " \n",
      "Reading the data.\n",
      "\n",
      "The current working directory is \"/Users/biona001/.julia/v0.6/IHT/docs/MendelIHT_tutorial\".\n",
      "\n",
      "Keywords modified by the user:\n",
      "\n",
      "  affected_designator = 2\n",
      "  control_file = gwas 1 Control nongen.txt\n",
      "  field_separator = \t\n",
      "  non_genetic_covariates = gwas 1 noncov.txt\n",
      "  pedigree_file = gwas 1 data.fam\n",
      "  plink_input_basename = gwas 1 data\n",
      "  predictors = 2\n",
      "  snpdata_file = gwas 1 data.bed\n",
      "  snpdefinition_file = gwas 1 data.bim\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mReading in data\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mv1.0 BED file detected\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mAnalyzing the data for model size k = 2\n",
      "\u001b[39m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IHT results:\n",
       "\n",
       "Compute time (sec):     0.979895101\n",
       "Final loss:             1183.6890803583544\n",
       "Iterations:             4\n",
       "Max number of groups:   1\n",
       "Max predictors/group:   2\n",
       "IHT estimated 2 nonzero coefficients.\n",
       "2×3 DataFrames.DataFrame\n",
       "│ Row │ Group │ Predictor │ Estimated_β │\n",
       "├─────┼───────┼───────────┼─────────────┤\n",
       "│ 1   │ 1     │ 3981      │ 0.149073    │\n",
       "│ 2   │ 1     │ 7023      │ 0.272991    │\n",
       "\n",
       "Intercept of model = 0.0\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using IHT\n",
    "MendelIHT(\"gwas 1 Control nongen.txt\") # change directory as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Observe that the resulting error and model is exactly the same, because the covariates we added is white noise, and thus, was not selected by IHT to be a significant predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Cross Validation\n",
    "\n",
    "In this example, users can run IHT on any number of model in at most 8 parallel threads. Empirically, running on $n$ threads achieves roughly $n/2$ fold speedup. Note that running $q$ fold cross validation on $r$ different models entails running IHT $q \\times r$ times. \n",
    "\n",
    "### Step 0: IMPORTANT in order to take advantage of multithreads, discontinue this notebook and julia. Have a terminal open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: IMPORTANT Execute following line in the terminal BEFORE starting notebook (or Julia REPL) \n",
    "export JULIA_NUM_THREADS=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Start notebook in the same terminal window and verify that notebook is indeed running with 8 threads:  \n",
    "Note that if you computer's capacity is less than 8, it will default to the largest number it can run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Specify the model sizes\n",
    "\n",
    "The paths should be inside quotes and separated by comma, specified via the keyword `model_sizes`. Each entry must be an integer. In this example, we tried to run IHT for model sizes $k = 1, 2, ..., 10$ and 5 different folds. This is equivalent to running IHT 50 different times, and hence, ideal for parallel computing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# Input and Output files.\n",
      "#\n",
      "plink_input_basename = gwas 1 data\n",
      "#\n",
      "# Cross Validation parameters\n",
      "#\n",
      "run_cross_validation = true\n",
      "cv_folds = 5\n",
      "model_sizes = \"1,2,3,4,5,6,7,8,9,10\"\n"
     ]
    }
   ],
   "source": [
    ";cat \"gwas 1 Control cv.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Cross Validation to find best model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "     Welcome to OpenMendel's\n",
      "      IHT analysis option\n",
      "        version 0.4.0\n",
      " \n",
      " \n",
      "Reading the data.\n",
      "\n",
      "The current working directory is \"/Users/biona001/.julia/v0.6/IHT/docs/MendelIHT_tutorial\".\n",
      "\n",
      "Keywords modified by the user:\n",
      "\n",
      "  affected_designator = 2\n",
      "  control_file = gwas 1 Control cv.txt\n",
      "  cv_folds = 5\n",
      "  model_sizes = 1,2,3,4,5,6,7,8,9,10\n",
      "  pedigree_file = gwas 1 data.fam\n",
      "  plink_input_basename = gwas 1 data\n",
      "  run_cross_validation = true\n",
      "  snpdata_file = gwas 1 data.bed\n",
      "  snpdefinition_file = gwas 1 data.bim\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mReading in data\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mv1.0 BED file detected\n",
      "\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "k\tMSE\n",
      "1\t0.5546784765054349\n",
      "2\t0.6004528460555323\n",
      "3\t0.6660062486666182\n",
      "4\t0.7124842256809767\n",
      "5\t0.7284745101298649\n",
      "6\t0.6771718567273666\n",
      "7\t0.6298517451998119\n",
      "8\t0.6174543457147491\n",
      "9\t0.6406876914640967\n",
      "10\t0.6628730245475413\n",
      "\n",
      "The lowest MSE is achieved at k = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mRunning 5-fold cross validation on the following model sizes:\n",
      "1,2,3,4,5,6,7,8,9,10.\n",
      "Ignoring keyword predictors.\n",
      "\u001b[39m"
     ]
    }
   ],
   "source": [
    "using IHT\n",
    "MendelIHT(\"gwas 1 Control cv.txt\") # change directory as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Re-run ordinary IHT on the best model size\n",
    "\n",
    "According to our cross validation result, the best model size that minimizes out-of-sample errors (i.e. MSE on the q-th subset of samples) is attained at $k = 1$. Using this information, one can re-run the IHT code to obtain the estimated model. Note importantly, that cross-validation in this case **did not** return the true model size $k = 2$. This reflects the general tendency for IHT to *under* estimate the true model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# Input and Output files.\n",
      "#\n",
      "plink_input_basename = gwas 1 data\n",
      "\n",
      "#\n",
      "# Analysis parameters for IHT option.\n",
      "#\n",
      "predictors = 1"
     ]
    }
   ],
   "source": [
    ";cat \"gwas 1 Control basic2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "     Welcome to OpenMendel's\n",
      "      IHT analysis option\n",
      "        version 0.4.0\n",
      " \n",
      " \n",
      "Reading the data.\n",
      "\n",
      "The current working directory is \"/Users/biona001/.julia/v0.6/IHT/docs/MendelIHT_tutorial\".\n",
      "\n",
      "Keywords modified by the user:\n",
      "\n",
      "  affected_designator = 2\n",
      "  control_file = gwas 1 Control basic2.txt\n",
      "  pedigree_file = gwas 1 data.fam\n",
      "  plink_input_basename = gwas 1 data\n",
      "  predictors = 1\n",
      "  snpdata_file = gwas 1 data.bed\n",
      "  snpdefinition_file = gwas 1 data.bim\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mReading in data\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mv1.0 BED file detected\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mAnalyzing the data for model size k = 1\n",
      "\u001b[39m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IHT results:\n",
       "\n",
       "Compute time (sec):     0.897766462\n",
       "Final loss:             1208.1108390895013\n",
       "Iterations:             3\n",
       "Max number of groups:   1\n",
       "Max predictors/group:   1\n",
       "IHT estimated 1 nonzero coefficients.\n",
       "1×3 DataFrames.DataFrame\n",
       "│ Row │ Group │ Predictor │ Estimated_β │\n",
       "├─────┼───────┼───────────┼─────────────┤\n",
       "│ 1   │ 1     │ 7023      │ 0.276275    │\n",
       "\n",
       "Intercept of model = 0.0\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MendelIHT(\"gwas 1 Control basic2.txt\") # change directory as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 (optional): Running different path sizes but don't run cross validation\n",
    "\n",
    "Instead of cutting your sample size into $q$ disjoint sets, IHT optionally allows users to run various models on all the samples. This happens if the user does not set `run_cross_validation` to true. An example is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# Input and Output files.\n",
      "#\n",
      "plink_input_basename = gwas 1 data\n",
      "#\n",
      "# Analysis parameters for IHT option.\n",
      "#\n",
      "predictors = 2\n",
      "model_sizes = \"1,2,3,4,5,6,7,8,9,10\"\n"
     ]
    }
   ],
   "source": [
    ";cat \"gwas 1 Control userpath.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      "     Welcome to OpenMendel's\n",
      "      IHT analysis option\n",
      "        version 0.4.0\n",
      " \n",
      " \n",
      "Reading the data.\n",
      "\n",
      "The current working directory is \"/Users/biona001/.julia/v0.6/IHT/docs/MendelIHT_tutorial\".\n",
      "\n",
      "Keywords modified by the user:\n",
      "\n",
      "  affected_designator = 2\n",
      "  control_file = gwas 1 Control userpath.txt\n",
      "  model_sizes = 1,2,3,4,5,6,7,8,9,10\n",
      "  pedigree_file = gwas 1 data.fam\n",
      "  plink_input_basename = gwas 1 data\n",
      "  predictors = 2\n",
      "  snpdata_file = gwas 1 data.bed\n",
      "  snpdefinition_file = gwas 1 data.bim\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mReading in data\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mv1.0 BED file detected\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mRunning the following model sizes: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\u001b[39m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0 0.0 … 0.140544 0.140545])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using IHT\n",
    "models, model_errors = MendelIHT(\"gwas 1 Control userpath.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** We can check that IHT is stable given different sparsity constraints, in the sense that parameters selected with a sparser constraint will still be selected if given a weaker constraint. Below we list the 10 models in separate columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9×10 Array{Float64,2}:\n",
       " 0.0       0.0       0.0       …  -0.0756696  -0.0766181  -0.0782086\n",
       " 0.0       0.0       0.0          -0.0780773  -0.077302   -0.0787067\n",
       " 0.0       0.149073  0.149073      0.145485    0.144214    0.143389 \n",
       " 0.0       0.0       0.0           0.08293     0.080625    0.0810577\n",
       " 0.0       0.0       0.0          -0.087082   -0.0869424  -0.0890821\n",
       " 0.0       0.0       0.0       …   0.0866336   0.0857555   0.0875377\n",
       " 0.0       0.0       0.0           0.0        -0.0737263  -0.0751238\n",
       " 0.276275  0.272991  0.272991      0.265817    0.264556    0.2656   \n",
       " 0.0       0.0       0.0           0.0         0.0        -0.0832995"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = find(models[:, end])\n",
    "full(models[idx, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 4: Using simulated results to examine IHT signal reconstruction\n",
    "\n",
    "In this example, we want to explicitly examine the ability for IHT to recover signals. Note the follow 2 examples directly call functions of `IHT.jl` instead of using control files. This allows greater flexibility on end users because they can directly manipulate variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Simulate data\n",
    "\n",
    "We first simulate a uniform random SNP matrix $X$ and a known model $\\beta$ as follows:\n",
    "$$\\beta \\sim N(0, 1),$$\n",
    "$$y = X\\beta + \\epsilon$$\n",
    "$$\\epsilon \\in N(0, s),$$\n",
    "$$s \\in \\{0.1, 1, 10, 25\\}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load packages\n",
    "using IHT\n",
    "using SnpArrays\n",
    "using DataFrames\n",
    "using Distributions\n",
    "\n",
    "#set random seed\n",
    "srand(1111) \n",
    "\n",
    "#specify dimension and noise of data\n",
    "n = 5000                        # number of cases\n",
    "p = 30000                       # number of predictors\n",
    "k = 10                          # number of true predictors per group\n",
    "S = [0.1, 1.0, 10.0, 25.0]      # noise vector, from very little noise to a lot of noise\n",
    "\n",
    "#construct snpmatrix, covariate files, and true model b\n",
    "x           = SnpArray(rand(0:2, n, p))    # a random snpmatrix\n",
    "z           = ones(n, 1)                   # non-genetic covariates, just the intercept\n",
    "true_b      = zeros(p)                     # model vector\n",
    "true_b[1:k] = randn(k)                     # Initialize k non-zero entries in the true model\n",
    "shuffle!(true_b)                           # Shuffle the entries\n",
    "correct_position = find(true_b)            # keep track of what the true entries are\n",
    "noise = [rand(Normal(0, s), n) for s in S] # noise vectors from N(0, s) where s ∈ S = {0.01, 0.1, 1, 10}s\n",
    "\n",
    "#compute mean and std used to standardize data to mean 0 variance 1\n",
    "mean_vec, minor_allele, = summarize(x)\n",
    "for i in 1:p\n",
    "    minor_allele[i] ? mean_vec[i] = 2.0 - 2.0mean_vec[i] : mean_vec[i] = 2.0mean_vec[i]\n",
    "end\n",
    "std_vec = std_reciprocal(x, mean_vec)\n",
    "\n",
    "#simulate phenotypes under different noises by: y = Xb + noise \n",
    "y = [zeros(n) for i in 1:length(S)]\n",
    "for i in 1:length(S)\n",
    "    SnpArrays.A_mul_B!(y[i], x, true_b, mean_vec, std_vec)\n",
    "    y[i] .+= noise[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe the noisiness of our observations. \n",
    "Each column is the same response vector with varying levels of noise added. The farther right, the noisier the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000×4 Array{Float64,2}:\n",
       " -2.99018    -2.22532    -1.82439    6.86836\n",
       "  0.0639139  -0.369665    5.09387    5.69815\n",
       "  0.842478    0.752519   18.7169   -28.0052 \n",
       " -2.82943    -3.8914      9.95096    4.61981\n",
       " -1.67051    -0.116633   -4.05996  -33.7148 \n",
       " -0.514721   -1.95655    -6.08555  -36.0566 \n",
       " -0.801959   -1.37353    -7.47094  -16.0338 \n",
       "  2.96974     5.17726    -8.5696   -12.3472 \n",
       "  3.10826     2.3005      7.51206  -33.1643 \n",
       " -0.464412    0.4904     -5.58351   16.1176 \n",
       " -0.616762   -0.790245   10.3874   -21.1504 \n",
       "  0.626816    0.374474   -8.41267   31.0773 \n",
       " -2.15693    -1.69911   -10.2978    23.1543 \n",
       "  ⋮                                         \n",
       " -0.848227   -1.33377     6.24567  -25.5871 \n",
       "  1.77269     2.75435     1.8081   -14.2573 \n",
       "  3.28046     2.37263   -10.7767     2.74097\n",
       "  2.53484     3.85943   -14.6097   -27.4922 \n",
       " -0.28382     1.24455     6.40927  -20.1135 \n",
       "  2.57121     3.26019    10.9377   -28.2579 \n",
       "  0.720075    0.407501  -14.2108   -24.9081 \n",
       " -1.00212    -0.294159    2.86219  -22.3564 \n",
       "  1.99847     2.29386   -13.6361    -3.32973\n",
       " -1.07315    -1.60252    -6.62297   17.6574 \n",
       " -4.83795    -5.04249    -8.24915  -36.6602 \n",
       " -0.942096   -0.998835    7.73013   45.3044 "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[y[1] y[2] y[3] y[4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Examine reconstruction of IHT given true model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>correct_position</th><th>true_β</th><th>noise_level_1</th><th>noise_level_2</th><th>noise_level_3</th><th>noise_level_4</th></tr></thead><tbody><tr><th>1</th><td>2012</td><td>0.20775</td><td>0.208856</td><td>0.205502</td><td>0.0</td><td>0.0</td></tr><tr><th>2</th><td>5685</td><td>0.372181</td><td>0.373598</td><td>0.357529</td><td>0.629893</td><td>0.0</td></tr><tr><th>3</th><td>7835</td><td>0.416006</td><td>0.414099</td><td>0.401277</td><td>0.0</td><td>0.0</td></tr><tr><th>4</th><td>9271</td><td>-0.160673</td><td>-0.161993</td><td>-0.168774</td><td>0.0</td><td>0.0</td></tr><tr><th>5</th><td>13188</td><td>-0.545489</td><td>-0.546157</td><td>-0.526095</td><td>0.0</td><td>0.0</td></tr><tr><th>6</th><td>19042</td><td>-0.695873</td><td>-0.696818</td><td>-0.714161</td><td>-0.672233</td><td>0.0</td></tr><tr><th>7</th><td>21966</td><td>-0.202504</td><td>-0.202652</td><td>-0.217099</td><td>0.0</td><td>0.0</td></tr><tr><th>8</th><td>23982</td><td>-0.489064</td><td>-0.489366</td><td>-0.495835</td><td>-0.56097</td><td>0.0</td></tr><tr><th>9</th><td>24758</td><td>0.551161</td><td>0.553689</td><td>0.529186</td><td>0.0</td><td>0.0</td></tr><tr><th>10</th><td>29481</td><td>-1.56535</td><td>-1.56519</td><td>-1.55339</td><td>-1.73005</td><td>-1.79669</td></tr></tbody></table>"
      ],
      "text/plain": [
       "10×6 DataFrames.DataFrame. Omitted printing of 2 columns\n",
       "│ Row │ correct_position │ true_β    │ noise_level_1 │ noise_level_2 │\n",
       "├─────┼──────────────────┼───────────┼───────────────┼───────────────┤\n",
       "│ 1   │ 2012             │ 0.20775   │ 0.208856      │ 0.205502      │\n",
       "│ 2   │ 5685             │ 0.372181  │ 0.373598      │ 0.357529      │\n",
       "│ 3   │ 7835             │ 0.416006  │ 0.414099      │ 0.401277      │\n",
       "│ 4   │ 9271             │ -0.160673 │ -0.161993     │ -0.168774     │\n",
       "│ 5   │ 13188            │ -0.545489 │ -0.546157     │ -0.526095     │\n",
       "│ 6   │ 19042            │ -0.695873 │ -0.696818     │ -0.714161     │\n",
       "│ 7   │ 21966            │ -0.202504 │ -0.202652     │ -0.217099     │\n",
       "│ 8   │ 23982            │ -0.489064 │ -0.489366     │ -0.495835     │\n",
       "│ 9   │ 24758            │ 0.551161  │ 0.553689      │ 0.529186      │\n",
       "│ 10  │ 29481            │ -1.56535  │ -1.56519      │ -1.55339      │"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute IHT result for less noisy data\n",
    "estimated_models = [zeros(k) for _ in 1:length(y)]\n",
    "for i in 1:length(y)\n",
    "    v = IHTVariables(x, z, y[i], 1, k)\n",
    "    result = L0_reg(v, x, z, y[i], 1, k)\n",
    "    estimated_models[i] .= result.beta[correct_position]\n",
    "end\n",
    "\n",
    "#compare and contrast\n",
    "true_model = true_b[correct_position]\n",
    "compare_model = DataFrame(\n",
    "    correct_position = correct_position, \n",
    "    true_β           = true_model, \n",
    "    noise_level_1    = estimated_models[1],      #N(0, 0.1)\n",
    "    noise_level_2    = estimated_models[2],      #N(0, 1.0)\n",
    "    noise_level_3    = estimated_models[3],      #N(0, 10.0)\n",
    "    noise_level_4    = estimated_models[4])      #N(0, 25.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Interpret reconstruction result\n",
    "\n",
    "IHT finds the correct 10 predictors if we add little noise. With greater noise, we lose predictors with smaller effect size, while losing accuracy. However, found predictors exhibit no shrinkage of effect size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 5: Using simulated results to examine IHT Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Follow steps 0~2 in example 3 to gain access to multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads() # check that multiple threads are running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Simulate Data (as done in Example 4 step 1)\n",
    "\n",
    "### Step 2: Run IHT cross validation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "k\tMSE\n",
      "2\t3.1677822019919044\n",
      "4\t2.8842864963598736\n",
      "6\t2.831063934427872\n",
      "8\t1.8713059507183205\n",
      "10\t2.257791657013664\n",
      "12\t2.2743936672699987\n",
      "14\t2.2656904985204647\n",
      "16\t2.263997497534819\n",
      "18\t2.2599121194306795\n",
      "20\t2.2535873590855027\n",
      "\n",
      "The lowest MSE is achieved at k = 8\n"
     ]
    }
   ],
   "source": [
    "srand(1111) #reset seed\n",
    "\n",
    "#run cross validation on not-so-noisy data\n",
    "path      = collect(2:2:20) \n",
    "num_folds = 3\n",
    "folds     = rand(1:num_folds, n)\n",
    "cv_iht(x, z, y[1], 1, path, folds, num_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Crossvalidation Results:\n",
      "k\tMSE\n",
      "2\t53.6730684917142\n",
      "4\t53.65755181959526\n",
      "6\t53.93280154016717\n",
      "8\t54.69513090164977\n",
      "10\t54.86837034931569\n",
      "12\t53.55242920512205\n",
      "14\t53.83447900670268\n",
      "16\t55.6527829783363\n",
      "18\t56.986528750175154\n",
      "20\t57.14268507459773\n",
      "\n",
      "The lowest MSE is achieved at k = 12\n"
     ]
    }
   ],
   "source": [
    "srand(1111) #reset seed\n",
    "\n",
    "#run cross validation on quite noisy data\n",
    "path      = collect(2:2:20) \n",
    "num_folds = 3\n",
    "folds     = rand(1:num_folds, n)\n",
    "cv_iht(x, z, y[3], 1, path, folds, num_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Interpreting results (recall $k_{\\text{true}} = 10$)\n",
    "\n",
    "When noise $\\epsilon \\in N(0, 0.1)$, cross-validation slightly underestimates the true sparsity parameter: $k_{\\text{estimate}} = 8$. For very noisy data $\\epsilon \\in N(0, 10)$, cross validation returns $k_{\\text{estimate}} = 12$, which slightly over-estimates the true sparsity parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook demonstrated some of the basic features of IHT. For more examples (particularly on how to use IHT on numeric data), readers can visit our older notebooks [here](https://github.com/klkeys/IHT.jl/blob/master/docs/IHT_tutorial_full.ipynb) and [here](https://github.com/klkeys/IHT.jl/blob/master/docs/IHT_tutorial_brief.ipynb). The first 3 examples illustrate the basic usage of IHT for general audiences. The last 2 examples illustrate how to make function calls directly and tests its robustness on reproducible simulated data. Notably, we selectively omitted several experimental features of IHT. In the near future, we will release a more detailed notebook that includes tutorials on all the experiemental features of IHT on GWAS data as well as motivations for them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.4",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
